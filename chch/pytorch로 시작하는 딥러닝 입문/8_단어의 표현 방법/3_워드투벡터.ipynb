{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 분산 표현(Distributed Representation)\r\n",
    "* 분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법\r\n",
    "*  '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'라는 가정\r\n",
    "*  희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현\r\n",
    "* NNLM, RNNLM 등이 있으나 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰이고 있다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [Word2Vec = CBOW / Skip-Gram]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CBOW(Continuous Bag of Words)\r\n",
    "* 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법  \r\n",
    "  \r\n",
    "    \r\n",
    "예문 : \"The fat cat sat on the mat\"  \r\n",
    "->  {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"}으로부터 sat을 예측  \r\n",
    "  \r\n",
    "  \r\n",
    "이 때 예측해야하는 단어 sat을 중심 단어(center word)라고 하고, 예측에 사용되는 단어들을 주변 단어(context word)라고 합니다. 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정했다면 이 범위를 윈도우(window)라고 합니다. 예를 들어서 윈도우 크기가 2이고, 예측하고자 하는 중심 단어가 sat이라고 한다면 앞의 두 단어인 fat와 cat, 그리고 뒤의 두 단어인 on, the를 참고합니다. 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n이 될 것입니다.  \r\n",
    "  \r\n",
    "    \r\n",
    "   <img src=\"https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG\" width=\"400\" height=\"400\">    \r\n",
    "     \r\n",
    "윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만들 수 있는데, 이 방법을 슬라이딩 윈도우(sliding window)라고 합니다.  \r\n",
    "  \r\n",
    "위의 그림은 윈도우 크기가 2일때 어떤식으로 이루어지는 지 보여준다.  \r\n",
    "### * 빨간색이 중심단어, 파란색이 주변 단어*\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  <img src=\"https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG\" width=\"400\" height=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* CBOW의 인공 신경망을 간단히 도식화하면 위와 같습니다.  \r\n",
    "* 입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고, 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 필요합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# word2vec은 딥러닝 모델이 아니다!\r\n",
    "은닉층(hidden Layer)이 1개인 경우에는 일반적으로 심층신경망(Deep Neural Network)이 아니라 얕은신경망(Shallow Neural Network)이라고 부릅니다.  \r\n",
    "또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 일반적인 은닉층과 구분하기 위해 투사층(projection layer)이라고 부르기도 합니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " <img src=\"https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG\" width=\"400\" height=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 투사층의 크기가 M이다.CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원이 된다. 위의 그림에서 투사층의 크기는 M=5이기 때문에 CBOW를 수행하고나서 얻는 각 단어의 임베딩 벡터의 차원은 5가 될 것이다.  \r\n",
    "2. 두번째는 입력층과 투사층 사이의 가중치 W는 V × M 행렬이며, 투사층에서 출력층사이의 가중치 W'는 M × V 행렬이다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}