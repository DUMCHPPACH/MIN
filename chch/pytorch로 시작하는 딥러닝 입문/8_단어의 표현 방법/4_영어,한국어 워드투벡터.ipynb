{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 04. 영어/한국어 Word2Vec 훈련시키기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "import urllib.request\r\n",
    "import zipfile\r\n",
    "from lxml import etree\r\n",
    "import re\r\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sswwd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x172e67236a0>)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\r\n",
    "# 저자의 경우 윈도우 바탕화면에서 작업하여서 'C:\\Users\\USER\\Desktop\\ted_en-20160408.xml'이 해당 파일의 경로.  \r\n",
    "target_text = etree.parse(targetXML)\r\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\r\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\r\n",
    "\r\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\r\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\r\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\r\n",
    "\r\n",
    "sent_text = sent_tokenize(content_text)\r\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\r\n",
    "\r\n",
    "normalized_text = []\r\n",
    "for string in sent_text:\r\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\r\n",
    "     normalized_text.append(tokens)\r\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\r\n",
    "\r\n",
    "result = []\r\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\r\n",
    "\r\n",
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for line in result[:3]: # 샘플 3개만 출력\r\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Word2Vec 훈련시키기\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "!pip install gensim"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim) (1.21.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sswwd\\anaconda3\\envs\\chch\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Installing collected packages: Cython, gensim\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\r\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "여기서 Word2Vec의 하이퍼파라미터값은 다음과 같습니다.\r\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\r\n",
    "window = 컨텍스트 윈도우 크기\r\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\r\n",
    "workers = 학습을 위한 프로세스 수\r\n",
    "sg = 0은 CBOW, 1은 Skip-gram."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model_result = model.wv.most_similar(\"man\")\r\n",
    "print(model_result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.8504380583763123), ('guy', 0.8252115845680237), ('lady', 0.7844919562339783), ('boy', 0.7633700966835022), ('girl', 0.7441701889038086), ('gentleman', 0.7377076148986816), ('soldier', 0.7263277173042297), ('kid', 0.6923007965087891), ('poet', 0.6832286715507507), ('person', 0.6540415287017822)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 한국어 word2vec 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. https://dumps.wikimedia.org/kowiki/latest/ 에 들어가서   \r\n",
    "kowiki-latest-pages-articles.xml.bz2 파일 다운로드(약 40분걸림)  \r\n",
    "\r\n",
    "2. https://github.com/attardi/wikiextractor 들어가서 code를 zip으로 다운로드  \r\n",
    "\r\n",
    "3. 압축 풀고 cmd에서 파일 있는 경로로 가서 가상환경 설정해주고 'python setdup.py install'입력  \r\n",
    "\r\n",
    "4. wikiextractor 폴더 안에 WikiExtractor.py 파일이 생기는데   \r\n",
    "동일한 경로 안에 kowiki-latest-pages-articles.xml.bz2 넣어준다.  \r\n",
    "* A:\\chchdata\\data\\wikiextractor-master\\wikiextractor\\WikiExtractor.py  \r\n",
    "\r\n",
    "* A:\\chchdata\\data\\wikiextractor-master\\wikiextractor\\kowiki-latest-pages-articles.xml.bz2\r\n",
    "5. cmd에 위의 경로로 들어가서 python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2 입력\r\n",
    "\r\n",
    "A:\\chch\\chch\\pytorch로 시작하는 딥러닝 입문\\8_단어의 표현 방법\\4_영어,한국어 워드투벡터.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}