{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. 파이토치 제공 데이터 사용\r\n",
    "2. 같은 클래스 별 폴더 이미지 데이터 이용\r\n",
    "3. 개인 데이터 사용(2 types)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import torchvision\r\n",
    "import torchvision.transforms as tr # 데이터를 불러오면서 전처리를 바로 할 수 있게 불러와주는 라이브러리\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transf = tr.Compose([tr.Resize(8),tr.ToTensor()])\r\n",
    "# 전처리 작업을 Compose 안에 일렬로 해준다.\r\n",
    "# compose = 순서대로 작업해준다.\r\n",
    "# "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='A:\\chch\\chchdata\\data', train=True, download=True, transform=transf)\r\n",
    "testset = torchvision.datasets.CIFAR10(root='A:\\chch\\chchdata\\data', train=False, download=True, transform=transf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "trainset[0][0].size()\r\n",
    "# cifar10이 rgb 채널이라 3, 8, 8이 된다.\r\n",
    "# 파이토치는 채널 수가 앞에 위치한다."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\r\n",
    "testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)\r\n",
    "# num_workers =  데이터로드를 할 때 서브 프로세스를 몇 개를 쓸 것인지 설정 -> 파라미터 튜닝에 해당\r\n",
    "# 고려 사항 : 학습 환경의 GPU개수, CPU개수, I/O 속도, 메모리 등"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "len(trainloader)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "dataiter = iter(trainloader) # 데이터를 하나씩 불러온다는 것\r\n",
    "images, labels = dataiter.next()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "images.size()\r\n",
    "#torch.Size([50, 3, 8, 8]) -> 배치사이즈, 채널 수 , 이미지 너비, 이미지 높이\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 같은 클래스 별 폴더 이미지 데이터 이용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "transf = tr.Compose([tr.Resize(16), tr.ToTensor()])\r\n",
    "trainset = torchvision.datasets.ImageFolder(root='A:\\\\chch\\\\chchdata\\\\data\\\\gender', transform=transf) \r\n",
    "# './class' 안에 있는 이미지를 자동으로 알아서 찾아주고, 각각의 다른 폴더에 대해 라벨링까지 해준다. transform을 설정해 전처리도 바로 할 수 있다.\r\n",
    "trainloader = DataLoader(trainset, batch_size=10, shuffle=False, num_workers=2)\r\n",
    "print(len(trainloader))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "trainset[0][0].size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 개인 데이터 사용(2 types)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# import preprocessing\r\n",
    "train_images = np.random.randint(256,size=(20,32,32,3))\r\n",
    "train_labels = np.random.randint(2, size=(20,1))\r\n",
    "\r\n",
    "# preprocessing..\r\n",
    "# train_images, train_labels = preprocessing(train_images, train_labels)\r\n",
    "print(train_images.shape, train_labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20, 32, 32, 3) (20, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TensorData(Dataset):\r\n",
    "    def __init__(self, x_data, y_data): # 외부데이터를 받기 위해 x_data, y_data 추가\r\n",
    "        self.x_data = torch.FloatTensor(x_data)\r\n",
    "        self.x_data = self.x_data.permute(0,3,1,2) \r\n",
    "        # 이미지 개수, 채널 수 , 이미지 너비, 이미지 높이 -> 순서를 바꾸기 위해 permute 사용\r\n",
    "        # (20, 32, 32, 3) -> (20, 3, 32, 32)\r\n",
    "        self.y_data = torch.LongTensor(y_data)\r\n",
    "        self.len = self.y_data.shape[0]\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.x_data[index], self.y_data[index]\r\n",
    "    def __len__(self):\r\n",
    "        return self.len"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "a64de8b745ab094eb3381810b1d090f0053b4977cc21c07d5a367fb401258f96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}